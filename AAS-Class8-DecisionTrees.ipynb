{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Decision Trees\n",
    "\n",
    "*(Partially retrieved from https://stackabuse.com/decision-trees-in-python-with-scikit-learn/)*\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "A decision tree is one of most frequently and widely used supervised machine learning algorithms that can perform both regression and classification tasks. The intuition behind the decision tree algorithm is simple, yet also very powerful.\n",
    "\n",
    "For each attribute in the dataset, the decision tree algorithm forms a node, where the most important attribute is placed at the root node. For evaluation we start at the root node and work our way down the tree by following the corresponding node that meets our condition or \"decision\". This process continues until a leaf node is reached, which contains the prediction or the outcome of the decision tree.\n",
    "\n",
    "This may sound a bit complicated at first, but what you probably don't realize is that you have been using decision trees to make decisions your entire life without even knowing it. Consider a scenario where a person asks you to lend them your car for a day, and you have to make a decision whether or not to lend them the car. There are several factors that help determine your decision, some of which have been listed below:\n",
    "\n",
    "1. Is this person a close friend or just an acquaintance? If the person is just an acquaintance, then decline the request; if the person is friend, then move to next step.\n",
    "2. Is the person asking for the car for the first time? If so, lend them the car, otherwise move to next step.\n",
    "3. Was the car damaged last time they returned the car? If yes, decline the request; if no, lend them the car.\n",
    "\n",
    "The decision tree for the aforementioned scenario looks like this:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/stackabuse/media/decision-trees-python-scikit-learn-1.png\" width=\"500\">\n",
    "\n",
    "### CART Model Representation\n",
    "The representation for the CART model is a binary tree.\n",
    "\n",
    "This is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n",
    "\n",
    "The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.\n",
    "\n",
    "Given a new input, the tree is traversed by evaluating the specific input started at the root node of the tree.\n",
    "\n",
    "#### Some **advantages** of decision trees are:\n",
    "* Simple to understand and to interpret. Trees can be visualised.\n",
    "* Requires little data preparation. \n",
    "* Able to handle both numerical and categorical data.\n",
    "* Possible to validate a model using statistical tests. \n",
    "* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "* They're very fast and efficient compared to KNN and other classification algorithms.\n",
    "\n",
    "#### The **disadvantages** of decision trees include:\n",
    "* Overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "* Decision trees can be unstable. Mitigant: Use decision trees within an ensemble.\n",
    "* Cannot guarantee to return the globally optimal decision tree. Mitigant: Training multiple trees in an ensemble learner\n",
    "* Decision tree learners create biased trees if some classes dominate. Recommendation: Balance the dataset prior to fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Decision Trees with Python Scikit Learn\n",
    "In this section, we will implement the decision tree algorithm using Python's Scikit-Learn library. In the following examples we'll solve both classification as well as regression problems using the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Decision Tree for Classification\n",
    "\n",
    "In this section we will predict whether a bank note is authentic or fake depending upon the four different attributes of the image of the note. The attributes are Variance of wavelet transformed image, curtosis of the image, entropy, and skewness of the image.\n",
    "\n",
    "For more detailed information about this dataset, check out the UCI ML repo for this dataset (https://archive.ics.uci.edu/ml/datasets/banknote+authentication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the steps to implement this algorithm in Scikit-Learn are identical to any typical machine learning problem, we will import libraries and datasets, perform some data analysis, divide the data into training and testing sets, train the algorithm, make predictions, and finally we will evaluate the algorithm's performance on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the Dataset**\n",
    "\n",
    "Since our file is in CSV format, we will use panda's read_csv method to read our CSV data file. Execute the following script to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"bill_authentication.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Analysis**\n",
    "\n",
    "Execute the following command to see the number of rows and columns in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will show \"(1372,5)\", which means that our dataset has 1372 records and 5 attributes.\n",
    "\n",
    "Execute the following command to inspect the first five records of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable here will be the column ``Class`` which values are only:\n",
    "- 0: Authentic\n",
    "- 1: Fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the Data**\n",
    "\n",
    "In this section we will divide our data into attributes and labels and will then divide the resultant data into both training and test sets. By doing this we can train our algorithm on one set of data and then test it out on a completely different set of data that the algorithm hasn't seen yet. This provides you with a more accurate view of how your trained algorithm will actually perform.\n",
    "\n",
    "To divide data into attributes and labels, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('Class', axis='columns')\n",
    "y = dataset['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the ``X`` variable contains all the columns from the dataset, except the ``Class`` column, which is the label. The ``y`` variable contains the values from the ``Class`` column. The ``X`` variable is our attribute set and ``y`` variable contains corresponding labels.\n",
    "\n",
    "The final preprocessing step is to divide our data into training and test sets. The ``model_selection`` library of Scikit-Learn contains ``train_test_split`` method, which we'll use to randomly split the data into training and testing sets. Execute the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the ``test_size`` parameter specifies the ratio of the test set, which we use to split up 20% of the data in to the test set and 80% for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Making Predictions**\n",
    "\n",
    "Once the data has been divided into the training and testing sets, the final step is to train the decision tree algorithm on this data and make predictions. Scikit-Learn contains the ``tree library``, which contains built-in classes/methods for various decision tree algorithms. Since we are going to perform a classification task here, we will use the ``DecisionTreeClassifier`` class for this example. The ``fit`` method of this class is called to train the algorithm on the training data, which is passed as parameter to the ``fit`` method. Execute the following script to train the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "algorithm = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "model = algorithm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "features = list(X.columns.values)\n",
    "classes = [\"Authentic\", \"Fake\"]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plot_tree(model,\n",
    "               feature_names=features,\n",
    "               class_names=classes,\n",
    "               fontsize=8, \n",
    "               filled=True, \n",
    "               rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our classifier has been trained, let's make predictions on the test data. To make predictions, the predict method of the ``DecisionTreeClassifier`` class is used. Take a look at the following code for usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Algorithm**\n",
    "\n",
    "At this point we have trained our algorithm and made some predictions. Now we'll see how accurate our algorithm is. For classification tasks some commonly used metrics are confusion matrix, precision, recall, and F1 score. Lucky for us Scikit-Learn's metrics library contains the ``classification_report`` and ``confusion_matrix`` methods that can be used to calculate these metrics for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "confusion = pd.DataFrame(cm, index=['authentic', 'fake'],\n",
    "                         columns=['predicted authentic','predicted fake'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW METHOD\n",
    "cm_fig = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['authentic', 'fake'])\n",
    "cm_fig.plot(cmap='Reds')\n",
    "\n",
    "# ANOTHER METHOD\n",
    "#import seaborn as sns\n",
    "#sns.heatmap(confusion, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, you can see that out of 275 test instances, our algorithm misclassified only 7. This is 97 % accuracy. Not too bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decision Tree for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of solving regression problem with decision tree using Scikit Learn is very similar to that of classification. However for regression we use DecisionTreeRegressor class of the tree library. Also the evaluation matrics for regression differ from those of classification. The rest of the process is almost same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "We will use this dataset to try and predict gas consumptions (in millions of gallons) in 48 US states based upon gas tax (in cents), per capita income (dollars), paved highways (in miles) and the proportion of population with a drivers license.\n",
    "\n",
    "The dataset is available at this link: https://people.sc.fsu.edu/~jburkardt/datasets/regression/x16.txt\n",
    "\n",
    "The first two columns in the above dataset do not provide any useful information, therefore they have been removed from the dataset file.\n",
    "\n",
    "Now let's apply our decision tree algorithm on this data to try and predict the gas consumption from this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('petrol_consumption.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see statistical details of the dataset, execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the Data**\n",
    "\n",
    "As with the classification task, in this section we will divide our data into attributes and labels and consequently into training and test sets.\n",
    "\n",
    "Execute the following commands to divide data into labels and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('Petrol_Consumption', axis=1)\n",
    "y = dataset['Petrol_Consumption']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the ``X`` variable contains all the columns from the dataset, except ``Petrol_Consumption`` column, which is the label. The ``y`` variable contains values from the ``Petrol_Consumption`` column, which means that the ``X`` variable contains the attribute set and ``y`` variable contains the corresponding labels.\n",
    "\n",
    "Execute the following code to divide our data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training and Making Predictions**\n",
    "\n",
    "As mentioned earlier, for a regression task we'll use a different sklearn class than we did for the classification task. The class we'll be using here is the ``DecisionTreeRegressor`` class, as opposed to the ``DecisionTreeClassifier`` from before.\n",
    "\n",
    "To train the tree, we'll instantiate the ``DecisionTreeRegressor`` class and call the fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "algo = DecisionTreeRegressor(random_state=3)\n",
    "\n",
    "model = algo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions on the test set, use the predict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test # 53.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Algorithm**\n",
    "\n",
    "To evaluate performance of the regression algorithm, the commonly used metrics are mean absolute error, mean squared error, and root mean squared error. The Scikit-Learn library contains functions that can help calculate these values for us. To do so, use this code from the metrics package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('R2 Score: ', metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean absolute error for our algorithm is 51.0, which is less than **10 percent** of the mean of all the values in the ``Petrol_Consumption`` column. This means that our algorithm did a fine prediction job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
