{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fa5315-fb41-4796-9ab7-35c27be8420a",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2510573-15ac-4e79-9090-0192221d04f2",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12d2ec-2738-476a-907a-0868cec5f498",
   "metadata": {},
   "source": [
    "Finding the optimal tuning parameters for a machine learning problem can often be very difficult. We may encounter **overfitting**, which means our machine learning model trains too specifically on our training dataset and causes higher levels of error when applied to our test/holdout datasets. Or, we may run into **underfitting**, which means our model doesn’t train specifically enough to our training dataset. This also leads to higher levels of error when applied to test/holdout datasets.\r\n",
    "\r\n",
    "When conducting a normal train/test split for model training and testing, the model trains on a specific randomly selected portion of the data, validates on a separate set of data, then finally tests on a holdout dataset. In practice this could lead to some **issues**, especially when the size of the dataset is relatively **small**, because you could be eliminating a portion of observations that would be key to training an optimal model. Keeping a percentage of data out of the training phase, even if its **15–25%** still holds plenty of information that would otherwise help our model train more **effectively**.\r\n",
    "\r\n",
    "In comes a solution to our problem — **Cross Validation**. Cross validation works by splitting our dataset into random groups, holding one group out as the test, and training the model on the remaining groups. This process is repeated for each group being held as the test group, then the average of the models is used for the resulting model.\r\n",
    "\r\n",
    "One of the most common types of cross validation is **K-Fold Cross Validation**, where $k$ is the number of folds within the dataset. Using $k=5$ is a common first step and easy for demonstrations of this principle below:\r\n",
    "\r\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*zSlot50Mu-NDODADz3pz4g.png\" width=500>\r\n",
    "\r\n",
    "Here we see five iterations of the model, each of which treats a **different fold** as the **test set** and **trains on the other four folds**. Once all five iterations are complete the resulting iterations are **averaged** (mean) together creating the **final cross validation model**.\r\n",
    "\r\n",
    "While cross validation can greatly benefit model development, there is also an important **drawback** that should be considered when conducting cross validation. Because each iteration of the model, up to $k$ times, requires you to run the full model, it can get **computationally expensive** as your dataset gets larger and as the value of $k$ increases. \r\n",
    "\r\n",
    "For example, running a cross validation model of $k = 10$ on a dataset with **1 million** observations requires you to run **10 separate models**, each of which uses all **1 million** observations. This won’t really be an issue with **small datasets** as the compute time would be in the scale of minute but when working with **larger datasets** with sizes in scales of many $GB$ or $TB$, the time required will **significantly increase**.e.below: below.ly below.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227ce3b-8f5c-4ea5-85de-0804a57c6652",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81eb705-e18a-4c68-a17d-2cd0f16773ee",
   "metadata": {},
   "source": [
    "However, when creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n",
    "\n",
    "These hyperparameters might address model design questions such as:\n",
    "\n",
    "- What **degree** of polynomial features should I use for my linear model?\n",
    "- What should be the **maximum depth** allowed for my decision tree?\n",
    "- What should be the **minimum number of samples** required at a leaf node in my decision tree?\n",
    "- How many **trees** should I include in my random forest?\n",
    "- How many **neurons** should I have in my neural network layer?\n",
    "- How many **layers** should I have in my neural network?\n",
    "- What should I set my **learning rate** to for gradient descent?\n",
    "\n",
    "I want to be absolutely clear, hyperparameters are not model parameters and they cannot be directly trained from the data. Model parameters are learned during training when we optimize a loss function using something like **gradient descent**. The process for learning parameter values is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872cdb9-f4a7-4389-8e0c-258d06c4a9e5",
   "metadata": {},
   "source": [
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/11/Screen-Shot-2017-11-02-at-1.28.26-PM.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f97fc-72b7-434f-8f77-3e1d1d9b3a24",
   "metadata": {},
   "source": [
    "Whereas the model parameters specify how to transform the input data into the desired output, the hyperparameters define how our model is actually structured. Unfortunately, there's no way to calculate: **Which way should I update my hyperparameter to reduce the loss?** (ie. gradients) in order to find the optimal model architecture; thus, we generally resort to experimentation to figure out what works best.\r\n",
    "\r\n",
    "In general, this process includes:\r\n",
    "\r\n",
    "1. Define a model\r\n",
    "2. Define the range of possible values for all hyperparameters\r\n",
    "3. Define a method for sampling hyperparameter values\r\n",
    "4. Define a cross-validation method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e17f51-458a-4ded-a640-22f1138a9d96",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb4f37-2d91-4574-9bf6-8e30f06a3bd2",
   "metadata": {},
   "source": [
    "Recall that I previously mentioned that the hyperparameter tuning methods relate to how we sample possible model architecture candidates from the space of possible hyperparameter values. This is often referred to as \"searching\" the hyperparameter space for the optimum values. \r\n",
    "\n",
    "In the following visualization, the **x** and **y** dimensions represent two hyperparameters, and the **z** dimension represents the model's score (defined by some evaluation metric) for the architecture defined by the **x** and **y**\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/11/hyperparameter_space.png\" width=500>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd410bc5-033e-42c0-adf5-3ba9cab67832",
   "metadata": {},
   "source": [
    "If we had access to such a plot, choosing the ideal hyperparameter combination would be trivial. However, calculating such a plot at the granularity visualized above would be prohibitively expensive. Thus, we are left to blindly explore the hyperparameter space in hopes of locating the hyperparameter values which lead to the maximum score.\r\n",
    "\r\n",
    "For each method, I'll discuss how to search for the optimal structure of a Random Forest classifer:\r\n",
    "- How many **estimators** (ie. decision trees) should I use?\r\n",
    "- What should be the maximum allowable **depth** for each decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4a32f-4645-4598-8c82-5b48b03c77e2",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "Grid search is arguably the most basic hyperparameter tuning method. With this technique, we simply build a model for each possible combination of all of the hyperparameter values provided, evaluating each model, and selecting the architecture which produces the best results.\n",
    "\n",
    "For example, we would define a list of values to try for both ``n_estimators`` and ``max_depth``, and a grid search would build a model for each possible combination.\n",
    "\n",
    "Performing grid search over the defined hyperparameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0376e24-9d48-411f-9dee-7345aea9ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth = [3, 10, 20, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72eff7-1513-450c-9d08-01815c572ad5",
   "metadata": {},
   "source": [
    "Would yeld the following models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88d7cb-2358-4c66-b576-3f3169a6e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier(n_estimators=10, max_depth=3)\n",
    "RandomForestClassifier(n_estimators=10, max_depth=10)\n",
    "RandomForestClassifier(n_estimators=10, max_depth=20)\n",
    "RandomForestClassifier(n_estimators=10, max_depth=40)\n",
    "\n",
    "RandomForestClassifier(n_estimators=50, max_depth=3)\n",
    "RandomForestClassifier(n_estimators=50, max_depth=10)\n",
    "RandomForestClassifier(n_estimators=50, max_depth=20)\n",
    "RandomForestClassifier(n_estimators=50, max_depth=40)\n",
    "\n",
    "RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "RandomForestClassifier(n_estimators=100, max_depth=20)\n",
    "RandomForestClassifier(n_estimators=100, max_depth=40)\n",
    "\n",
    "RandomForestClassifier(n_estimators=200, max_depth=3)\n",
    "RandomForestClassifier(n_estimators=200, max_depth=10)\n",
    "RandomForestClassifier(n_estimators=200, max_depth=20)\n",
    "RandomForestClassifier(n_estimators=200, max_depth=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce84ded4-5c1a-4938-9660-f2929e7ae4fc",
   "metadata": {},
   "source": [
    "Each model would be fit to the training data and evaluated on the validation data. As you can see, this is an **exhaustive** sampling of the hyperparameter space and can be quite inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee15b2e-1bd2-4169-b203-c5983c9661b0",
   "metadata": {},
   "source": [
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/11/grid_search.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bae59-1a49-4f74-bf6d-1c1c40b25344",
   "metadata": {},
   "source": [
    "To implement the Grid-Search, we have a Scikit-Learn library called **GridSearchCV**.\n",
    "\n",
    "The computational time would be long, but it would reduce the manual efforts by avoiding the ‘n’ number of lines of code. Library itself perform the search operations and returns the performing model and its score. In which each model are built for each permutation of a given hyperparameter, internally it would be evaluated and ranked across the given cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615227d-8baf-4bf9-8106-6d37d6e4d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Step 1: Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20,\n",
    "                           n_informative=2, n_redundant=10,\n",
    "                           random_state=42, n_classes=2)\n",
    "\n",
    "# Step 2: Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [3, 10, 20, 40],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Step 3: Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Step 4: Use GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Step 5: Fit the model\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best mean Cross-validation Accuracy score: \", round(grid_search.best_score_, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6026660-580e-4bcd-984a-a4f882f874c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Initialize an empty array to store mean scores\n",
    "    scores_mean = np.zeros((len(grid_param_2), len(grid_param_1)))\n",
    "\n",
    "    # Iterate over all combinations and compute mean scores\n",
    "    for i, val1 in enumerate(grid_param_1):\n",
    "        for j, val2 in enumerate(grid_param_2):\n",
    "            # Filter results for specific n_estimators and max_depth\n",
    "            mask = (cv_results['param_' + name_param_1] == val1) & (cv_results['param_' + name_param_2] == val2)\n",
    "            # Average over all bootstrap and criterion combinations\n",
    "            scores_mean[j, i] = np.mean(cv_results['mean_test_score'][mask])\n",
    "\n",
    "    # Plotting\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        plt.plot(grid_param_1, scores_mean[idx, :], '-o', label=name_param_2 + ': ' + str(val))\n",
    "\n",
    "    plt.title(\"Grid Search Scores (Averaged over Bootstrap and Criterion)\")\n",
    "    plt.xlabel(name_param_1)\n",
    "    plt.ylabel('CV Average Score')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid('on')\n",
    "\n",
    "plot_grid_search(grid_search.cv_results_, param_grid['n_estimators'], param_grid['max_depth'], 'n_estimators', 'max_depth')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4432b-ff26-4b30-88a9-66a650f59e85",
   "metadata": {},
   "source": [
    "#### Random search\r\n",
    "Random search differs from grid search in that we longer provide a discrete set of values to explore for each hyperparameter; rather, we provide a statistical distribution for each hyperparameter from which values may be randomly sampled.\r\n",
    "\r\n",
    "We'll define a sampling distribution for e hyperparameter:er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8cce6-b44e-4eff-86ab-c8a6b9edaa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "min_estimators, max_estimators = 2, 400\n",
    "n_estimators = np.random.randint(min_estimators, max_estimators)\n",
    "low, high = 1, 40\n",
    "max_depth = np.random.randint(low, high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20415856-2305-41dd-8676-f34807830688",
   "metadata": {},
   "source": [
    "The Grid Search one that we have discussed above usually increases the complexity in terms of the computation flow, So sometimes GS is considered inefficient since it attempts all the combinations of given hyperparameters.  But the **Randomized Search** is used to train the models based on random hyperparameters and combinations. obviously, the number of training models are **smaller** than grid search.\r\n",
    "\r\n",
    "In simple terms, In Random Search, in a given grid, the list of hyperparameters are trained and test our model on a random combination of given hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7136cb5-fdb7-46e9-96f4-965574e5a8b9",
   "metadata": {},
   "source": [
    "<img src=\"https://www.jeremyjordan.me/content/images/2017/11/random_search.gif\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4d216-52bc-455d-a40e-bf80f74f3dbe",
   "metadata": {},
   "source": [
    "To implement the Random Search, we have a Scikit-Learn library called **RandomSearchCV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bf972-17fb-4f86-993a-71ad6cad60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20,\n",
    "                           n_informative=2, n_redundant=10,\n",
    "                           random_state=42, n_classes=2)\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': np.random.randint(2, 400, 100),  # 100 random values between 2 and 400\n",
    "    'max_depth': np.random.randint(1, 40, 100),       # 100 random values between 1 and 40\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Use RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distributions,\n",
    "                                   n_iter=100, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best mean Cross-validation Accuracy score: \", round(grid_search.best_score_, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35e7ac-92af-4b55-b495-4958c61f2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# Plot the results for RandomizedSearchCV\n",
    "def plot_random_search(cv_results, name_param_1, name_param_2):\n",
    "    scores = cv_results['mean_test_score']\n",
    "    param_1_values = cv_results['param_' + name_param_1]\n",
    "    param_2_values = cv_results['param_' + name_param_2]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(param_1_values, param_2_values, c=scores, cmap='viridis')\n",
    "    plt.colorbar(scatter, label='CV Average Score')\n",
    "    plt.xlabel(name_param_1)\n",
    "    plt.ylabel(name_param_2)\n",
    "    plt.title(f\"Random Search CV Scores ({name_param_1} vs {name_param_2})\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function\n",
    "plot_random_search(random_search.cv_results_, 'n_estimators', 'max_depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf6ccf-1d11-4492-86f7-eeed1aea7ea2",
   "metadata": {},
   "source": [
    "## Grid Search vs. Random Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db4a95-ecfb-4433-bfee-fd575026f477",
   "metadata": {},
   "source": [
    "|                  GridSearchCV                  |                       RandomSearshCV                      |\r\n",
    "|:----------------------------------------------:|:---------------------------------------------------------:|\r\n",
    "|              Grid is well-defined              |                  Grid is not well defined                 |\r\n",
    "|          Discrete values for HP-params         |       Continuos values and Statistical distribution       |\r\n",
    "|      Defined size for Hyperparameter space     |                   No such a restriction                   |\r\n",
    "|   Picks of the best combination from HP-Space  |             Picks up the samples from HP-Space            |\r\n",
    "|             Samples are not created            | Samples are created and specified by the range and n_iter |\r\n",
    "|            Low performance than RSCV           |               Better performance and result               |\r\n",
    "| Guided flow to search for the best combination |          The name itself says that, no guidance.          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a22e7-de6e-453f-aecc-13cc34a380b0",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    The blow pictorial representation would give you the best understanding of GridSearchCV and RandomSearshCV.\n",
    "    <br><br>\n",
    "    <img src=\"https://editor.analyticsvidhya.com/uploads/73200GSRS-CV.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee88e3-47bc-44bb-8a54-331dffc2133b",
   "metadata": {},
   "source": [
    "### Conclusion\r\n",
    "So far we have discussed in a detailed study of Hyperparameter visions with respect to the Machine Learning point of view, please remember a few things:\r\n",
    "\r\n",
    "- Each model has a set of hyperparameters, so we have carefully chosen them and tweaked them during hyperparameter tuning.\r\n",
    "- All hyperparameters are **NOT** equally important and no defined rules for this. Try to use continuous values instead of discrete values.\r\n",
    "- Make sure to use **K-Fold Cross Validation** while using Hyperparameter tuning to improvise your hyperparameter tuning and coverage of hyperparameter space.\r\n",
    "- Go with a better combination for hyperparameters and build strong results.ults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976f6de-53a9-4c2d-9680-42b3915829f7",
   "metadata": {},
   "source": [
    "### Hyperoptimization Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9ac37-7b55-4a39-8f14-4b5c04c4963d",
   "metadata": {},
   "source": [
    "- Ray.tune: Hyperparameter Optimization Framework - https://www.ray.io/\n",
    "- Optuna - https://optuna.org/\n",
    "- Hyperopt - https://github.com/hyperopt/hyperopt-sklearn\n",
    "- Polyaxon\n",
    "- Talos\n",
    "- Spearmint\n",
    "- GPyOpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
