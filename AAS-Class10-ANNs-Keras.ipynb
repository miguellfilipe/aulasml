{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAeVVlw71UGb"
   },
   "source": [
    "# Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4CPMSWZ1UGe"
   },
   "source": [
    "<a id='9'></a>\n",
    "# 1. Implementation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWbzB63R1UGe"
   },
   "source": [
    "A brief about the NN libraries:\n",
    "\n",
    "Theano is an open source numerical computation library based on numpy syntax. It can run not only on the CPU ( Central Processing Unit) but also the GPU (Graphical Processing Unit). GPU is a processor for graphic purposes somewhat similar to a graphic card. GPU is much more powerful in terms of efficiency etc. because it has more cores and is able to run more floating points calculations per second than the CPU. GPU is highly specialised for heavy, parallel computations which is a requirement in Neural Networks that we are about to see.\n",
    "\n",
    "How parallel computation comes into play in NNs? When we are forward propagating the different activations of neurons for the activation function or when we back propagate the error. Also calculations can be carried out faster this way. Theano was developed at the University of Montreal.\n",
    "\n",
    "Tensorflow is similar to Theano. It's been developed by Google.\n",
    "\n",
    "However the point to consider is that these two libraries are more towards the research and development side of Neural Networks. If we were to create a model from scratch and make some improvements in it, experiment or something these two would be great but right now we would be using Keras for beginning till we step up. Keras in some way wraps the two libraries for us and provides small and easy to implement modules of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "kTdqjh3N1UGe"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHgnUMOh1UGf"
   },
   "source": [
    "<a id='10'></a>\n",
    "# 2. Business Problem and EDA\n",
    "\n",
    "Our Business problem which I have chosen for this tutorial is a classification problem wherein we have a dataset in which there are details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xn_6SzSL1UGf"
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "churn_data = pd.read_csv('Churn_Modelling.csv', index_col='RowNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwkT-1wT1UGf"
   },
   "outputs": [],
   "source": [
    "churn_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zM9ImFX1UGf"
   },
   "outputs": [],
   "source": [
    "churn_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRRjhioG1UGg"
   },
   "outputs": [],
   "source": [
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKpXs8GW1UGg"
   },
   "outputs": [],
   "source": [
    "churn_data['Geography'].value_counts()\n",
    "# some columns are totally unproductive so let's remove them\n",
    "#churn_data.drop(['CustomerId','Surname'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TV8rU7Ud1UGg"
   },
   "outputs": [],
   "source": [
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78MInM0g1UGg"
   },
   "outputs": [],
   "source": [
    "# some columns have text data so let's one hot encode them\n",
    "#  for more on one hot encoding click this link below\n",
    "# https://www.kaggle.com/shrutimechlearn/types-of-regression-and-stats-in-depth\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# encode Gender column to 0 and 1\n",
    "churn_data_encoded = churn_data.copy()\n",
    "label_enconder = LabelEncoder()\n",
    "churn_data_encoded['Gender'] = label_enconder.fit_transform(churn_data_encoded['Gender'])\n",
    "\n",
    "# enconde Geography column to a binary array\n",
    "Geography_dummies = pd.get_dummies(prefix='Geo',data=churn_data_encoded,columns=['Geography'])\n",
    "\n",
    "# OR VIA LABEL ENCONDING and ONE HOT ENCODING\n",
    "\n",
    "#column_transformer = ColumnTransformer([(\"Geography\", OneHotEncoder(), [1])], remainder = 'passthrough')\n",
    "#churn_data_encoded = column_transformer.fit_transform(churn_data_encoded)\n",
    "#churn_data_encoded = churn_data_encoded[:, 1:]\n",
    "#churn_data_encoded = pd.DataFrame(churn_data_encoded)\n",
    "#churn_data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xt-3RCMm1UGg"
   },
   "outputs": [],
   "source": [
    "Geography_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwUOf0gN1UGg"
   },
   "outputs": [],
   "source": [
    "churn_data_encoded = Geography_dummies.copy()\n",
    "churn_data_encoded = churn_data_encoded.drop(['Surname', 'CustomerId'], axis = 1)\n",
    "churn_data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWNBEovH1UGg"
   },
   "outputs": [],
   "source": [
    "sns.countplot(y=churn_data_encoded.Exited ,data=churn_data_encoded, hue=churn_data_encoded.Exited)\n",
    "plt.xlabel(\"Count of each Target class\")\n",
    "plt.ylabel(\"Target classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaSznlem1UGg"
   },
   "outputs": [],
   "source": [
    "churn_data_encoded.hist(figsize=(15,12),bins = 15)\n",
    "plt.title(\"Features Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR_9UcDB1UGh"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "p=sns.heatmap(churn_data_encoded.corr(), annot=True,cmap='RdYlGn',center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lgr_xBIM1UGh"
   },
   "source": [
    "#### X and y definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qF35yzgj1UGh"
   },
   "outputs": [],
   "source": [
    "X = churn_data_encoded.drop('Exited',axis=1).values\n",
    "y = churn_data_encoded.Exited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olTbi8901UGh"
   },
   "source": [
    "#### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2iT-Hs01UGh"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_ggEGGx1UGh"
   },
   "source": [
    "#### Feature Scaling (Data Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74AOgRi31UGh"
   },
   "outputs": [],
   "source": [
    "# Feature Scaling because yes we don't want one independent variable dominating the other and it makes computations easy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUFyY39d1UGh"
   },
   "outputs": [],
   "source": [
    "# sequential model to initialise our ann and dense module to build the layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9kGZ8Wy1UGo"
   },
   "source": [
    "#### ANN Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxCSE9M41UGp"
   },
   "outputs": [],
   "source": [
    "ann = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "input_layer = Dense(units = 12, activation = 'relu', input_dim = 12)\n",
    "ann.add(input_layer)\n",
    "\n",
    "# Adding the hidden layer 1\n",
    "hidden_layer_1 = Dense(units = 6, activation = 'relu')\n",
    "ann.add(hidden_layer_1)\n",
    "\n",
    "# Adding the hidden layer 2\n",
    "hidden_layer_2 = Dense(units = 3, activation = 'relu')\n",
    "ann.add(hidden_layer_2)\n",
    "\n",
    "# Adding the output layer\n",
    "# Sigmoid -> Binary classification, Softmax -> Multiclassification\n",
    "ann.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Loss (Binary Classification) -> binary_crossentropy, Multiclass -> categorical_crossentropy\n",
    "# Compiling the ANN | means applying ADAM on the whole ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV2T7PKg1UGp"
   },
   "source": [
    "#### ANN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNmSIY-Z1UGp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "history = ann.fit(X_train, y_train, batch_size=50, epochs = 100,verbose = 1, validation_split=0.2, validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOzVHAkV1UGp"
   },
   "source": [
    "<a id='11'></a>\n",
    "# 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80uqwYK01UGp"
   },
   "source": [
    "#### Evaluating the generated ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGJoDBL01UGp"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss and accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss / accuracy convergence')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train Accuracy', 'Validation Accuracy', \"Train Loss\", \"Validation Loss\"], loc='center right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6UP_JJo1UGp"
   },
   "outputs": [],
   "source": [
    "score, acc = ann.evaluate(X_train, y_train, batch_size=10)\n",
    "print('Train score:', score)\n",
    "print('Train accuracy:', acc)\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "print('*'*20)\n",
    "score, acc = ann.evaluate(X_test, y_test, batch_size=10)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucutTFlN1UGp"
   },
   "outputs": [],
   "source": [
    "p = sns.heatmap(cm, annot=True, cmap=\"Blues\" ,fmt='g')\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WW1IQmye1UGr"
   },
   "outputs": [],
   "source": [
    "#import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhuBYKSY1UGr"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_predict_probability = ann.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_predict_probability)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr,tpr, label='ANN')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHas2sky1UGr"
   },
   "outputs": [],
   "source": [
    "#Area under ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_predict_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLB0qGkA1UGs"
   },
   "source": [
    "Deviation is very low so I'd say that it is unlikely to be an overfitted model. With different training sets it got the mean with all training results is still very close to the above model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FI9FOE91UGs"
   },
   "source": [
    "<a id='12'></a>\n",
    "# 4. Improving ANN with Dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYJp8zXb1UGs"
   },
   "source": [
    "Dropout Regularization is used to ignore certain neurons in order to reduce noise (overfitting) if needed.\n",
    "\n",
    "<img src=\"https://preview.ibb.co/e7yPPp/dropout.jpg\" alt=\"dropout\" border=\"0\">\n",
    "\n",
    "p is the fraction of input units to drop. If suppose there are ten neurons from a layer and p is 0.1 then one of the neurons would be disabled and its output would not be sent to the further layer.\n",
    "\n",
    "It is advisable to start with p 0.1 and move to higher values when in case the overfitting problem persists. Also going over 0.5 is not advisable generally because it may cause underfitting as most of the neurons are disabled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZjQe4Uf1UGs"
   },
   "source": [
    "#### Building our ANN with Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjiCSniG1UGs"
   },
   "outputs": [],
   "source": [
    "# Improving the ANN\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "ann = Sequential()\n",
    "# Adding the input layer and the first hidden layer (w/ Dropout)\n",
    "ann.add(Dense(units = 12, activation = 'relu', input_dim = 12))\n",
    "ann.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Adding the hidden layer 1 (w/ Dropout)\n",
    "ann.add(Dense(units = 24, activation = 'relu'))\n",
    "ann.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Adding the hidden layer 2 (w/ Dropout)\n",
    "ann.add(Dense(units = 12, activation = 'relu'))\n",
    "ann.add(Dropout(rate = 0.1))\n",
    "\n",
    "# Adding the output layer\n",
    "ann.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hc00QmZB1UGs"
   },
   "source": [
    "#### Training our ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSxTVqXy1UGs"
   },
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "history = ann.fit(X_train, y_train, batch_size=100, epochs = 200,verbose = 1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stMRziSG1UGs"
   },
   "source": [
    "#### Evaluation Plots and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOYZB0wA1UGt"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss and accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss / accuracy convergence')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train Accuracy', 'Validation Accuracy', \"Train Loss\", \"Validation Loss\"], loc='center right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rkBDhPq1UGt"
   },
   "outputs": [],
   "source": [
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "score, acc = ann.evaluate(X_train, y_train,\n",
    "                            batch_size=10)\n",
    "print('Train score:', score)\n",
    "print('Train accuracy:', acc)\n",
    "# Part 3 - Making predictions and evaluating the model\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print('*'*20)\n",
    "score, acc = ann.evaluate(X_test, y_test,\n",
    "                            batch_size=10)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpERDrC41UGt"
   },
   "outputs": [],
   "source": [
    "p = sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"Blues\" ,fmt='g')\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnqIj-fO1UGt"
   },
   "outputs": [],
   "source": [
    "#import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CI7Tc5mE1UGt"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_predict_probability = ann.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_predict_probability)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr,tpr, label='ANN')\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "942Qfdp41UGt"
   },
   "outputs": [],
   "source": [
    "#Area under ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_predict_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3DxNPyd1UGu"
   },
   "source": [
    "# 5. Single prediction exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E09Yp141UGu"
   },
   "source": [
    "Use our ANN model to predict if the customer with the following informations will leave the bank:\n",
    "\n",
    "- **Credit Score**: 600\n",
    "- **Gender**: Male\n",
    "- **Age**: 40 years old\n",
    "- **Tenure**: 3 years\n",
    "- **Balance**: \\$60.000\n",
    "- **Number of Products**: 2\n",
    "- **Does this customer have a credit card?**: Yes\n",
    "- **Is this customer an Active Member?**: Yes\n",
    "- **Estimated Salary**: \\$50.000\n",
    "- **Geography**: France\n",
    "\n",
    "So, should we say goodbye to that customer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPHX1afo1UGu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFdxmoF41UGu"
   },
   "source": [
    "**And now this client?**\n",
    "\n",
    "- **Credit Score**: 500\n",
    "- **Gender**: Female\n",
    "- **Age**: 65 years old\n",
    "- **Tenure**: 5 years\n",
    "- **Balance**: \\$160.000\n",
    "- **Number of Products**: 3\n",
    "- **Does this customer have a credit card?**: Yes\n",
    "- **Is this customer an Active Member?**: No\n",
    "- **Estimated Salary**: \\$100.000\n",
    "- **Geography**: Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gODDq6Z51UGu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
