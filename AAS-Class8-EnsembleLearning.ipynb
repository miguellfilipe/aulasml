{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9ba5b8",
   "metadata": {},
   "source": [
    "# 1. Ensemble Learning: Joining Forces\n",
    "*(partially retrieved from https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-23f9336d3cb0) and https://coding-blocks.github.io/DS-NOTES/7.%20Ensemble.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1f77c",
   "metadata": {},
   "source": [
    "Ensemble learning is a machine learning technique combining multiple individual models to create a stronger, more accurate predictive model. By leveraging the diverse strengths of different models, ensemble learning aims to mitigate errors, enhance performance, and increase the overall robustness of predictions, leading to improved results across various tasks in machine learning and data analysis.\n",
    "\n",
    "There are **two** types of learners in Ensemble Learning:\n",
    "\n",
    "- **Weak Learners** (single models): Individual models are known as weak learners. We call them weak learners because they either have a high bias or high variance.\n",
    "- **Strong Learners** (ensemble models): Strong learners are the combination of various different weak learners that allow for a more accurate final model.\n",
    "\n",
    "### Wisdom Of The Crowd\n",
    "\n",
    "Suppose you pose a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert's answer. This is called *Wisdom of The Crowd*. Similarly, if you aggregate the predictions of group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an *ensemle*; thus, this technique is called *Ensemble Learning*.  \n",
    " \n",
    "Suppose you have a dataset with 10 instances and you have to perform classification task. As per current knowledge, **which model will you use?**  \n",
    "  \n",
    "    \n",
    "Most probably you'll pick the one who will give the highest accuracy, right?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b84718fd",
   "metadata": {},
   "source": [
    "### It's a bird? It's a plane? No. It's Batman!\n",
    "\n",
    "Consider the fable of the blind men and the elephant depicted in the image below. The blind men are each describing an elephant from their own point of view. Their descriptions are all correct but incomplete. Their understanding of the elephant would be more accurate and realistic if they came together to discuss and combined their descriptions.\n",
    "\n",
    "<img src=\"https://pluralsight2.imgix.net/guides/27d18692-27b8-42aa-8aa5-9668e625c41c_1.jpg\" width=\"400\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fff3fc2",
   "metadata": {},
   "source": [
    "### Types of Ensemble Learning\n",
    "\n",
    "1. Bagging (Bootstrap Aggregation)\n",
    "    * Voting\n",
    "2. Boosting\n",
    "\n",
    "The main idea behind **Ensemble Learning** is the usage of multiple algorithms and models that are used together for the same task. While **single models** use only one algorithm (learner) to create prediction models. **Bagging** and **boosting** methods aim to combine several of those to achieve better prediction with higher consistency compared to individual learners.\n",
    "\n",
    "<img src=\"https://pluralsight2.imgix.net/guides/81232a78-2e99-4ccc-ba8e-8cd873625fdf_2.jpg\" width=\"500\">\n",
    "\n",
    "<!--<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*8vznmPx1HXTvfW5QzbnsCg.png\" width=\"600\">-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6844608b",
   "metadata": {},
   "source": [
    "### Monitoring Ensemble Learning Models\n",
    "Ensemble learning improves a model’s performance in mainly three ways:\n",
    "\n",
    "- By reducing the variance of weak learners\n",
    "- By reducing the bias of weak learners\n",
    "\n",
    "So,\n",
    "\n",
    "- **Bagging** can be used to **reduce the variance** of weak learners. \n",
    "\n",
    "- **Boosting** can be used to **reduce the bias** of weak learners.\n",
    "\n",
    "So when should we use it? Cleary, when we see **overfitting** or **underfitting** of our models.\n",
    "\n",
    "One must choose the ensemble method that most fits the problem.\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "The next chart might be familiar to you, but it represents quite well the relationship and the tradeoff between bias and variance on the test error rate.\n",
    "\n",
    "You might be familiar with the following concept, but I posit that it effectively illustrates the correlation and compromise between bias and variance with respect to the testing error rate.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*Za83whww3ucVQ7bDJq93AQ.png\" width=\"500\">\n",
    "\n",
    "The relationship between the variance and bias of a model is such that a **reduction in variance** results in an **increase in bias**, and **vice versa**. To **achieve optimal performance**, the model must be positioned at an equilibrium point, where the test error rate is **minimized**, and the variance and bias are appropriately **balanced**.\n",
    "\n",
    "### High-bias and High-variance Models\n",
    "\n",
    "The individual models that we combine can **often** (**NOT ALWAYS**) either have a **high bias** or **high variance**. Because they either have high bias or variance, weak learners cannot learn efficiently and can perform poorly.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/UpdVr.jpg\" width=\"400\">\n",
    "\n",
    "- A high-bias model results from **not learning data well enough** (underfit). It is not related to the distribution of the data. Hence future predictions will be unrelated to the data and thus incorrect.\n",
    "- A high variance model results from learning the data **too well**. It varies with each data point. Hence it is impossible to predict the next point accurately.\n",
    "\n",
    "Both high bias and high variance models thus cannot generalize properly. Thus, weak learners will either make incorrect generalizations or fail to generalize altogether. Because of this, the predictions of weak learners cannot be relied on by themselves.\n",
    "\n",
    "As we know from the bias-variance trade-off, an **underfit model** has **high bias and low variance**, whereas an **overfit** model has **high variance and low bias**. In either case, there is no balance between bias and variance. For there to be a balance, both the bias and variance need to be low. **Ensemble learning** tries to balance this bias-variance trade-off by reducing either the bias or the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137c08f",
   "metadata": {},
   "source": [
    "# 2. Bagging (Bootstrap Aggregation) - Reduce Variance\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use same training algorithm for every predictor and train them on different random subsets of the training set. Whem sampling is performed with replacement, this method is called ***Bagging*** (short for **Bootstrap Aggregation**).  \n",
    "  \n",
    "In other words, bagging allows **training instances** to be sampled several times for the same predictor.  \n",
    "\n",
    "<br>\n",
    "<img src=\"https://coding-blocks.github.io/DS-NOTES/_images/ensem2.png\" width=\"400\">\n",
    "<br>\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the *statistical mode* (the prediction that is most occuring) for classification, or the *statistical mean* (average) for regression.  \n",
    "  \n",
    "You can train these different models in **parallel systems**, via different CPU Cores or even different servers. Similarly, predictions can be made in parallel as well. This is one of the reasons why bagging is a very popular method: It scales very well.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*9B7b8kDs9IrgfdcbWC0DLA.png\" width=\"600\">\n",
    "\n",
    "The **Random Forest Classifier** belongs to **bagging** method since it is composed of multiple **Decision Tree Classifiers**.\n",
    "\n",
    "Bagging consists of **two steps**:\n",
    "\n",
    "## Step 1: Bootstrapping\n",
    "Involves resampling subsets of data with replacement from an initial dataset. In other words, subsets of data are taken from the initial dataset. These subsets of data are called bootstrapped datasets or, simply, bootstraps. Resampled ‘with replacement’ means an individual data point can be sampled multiple times. Each bootstrap dataset is used to train a weak learner.\n",
    "\n",
    "## Step 2: Aggregating\n",
    "The individual weak learners are trained independently from each other. Each learner makes independent predictions. The results of those predictions are aggregated at the end to get the overall prediction. The predictions are aggregated using either max voting or averaging.\n",
    "\n",
    "### Max Voting (Classification)\n",
    "It is a commonly used for classification problems that consists of taking the mode of the predictions (the most occurring prediction). It is called voting because like in election voting, the premise is that ‘the majority rules’. Each model makes a prediction. A prediction from each model counts as a single ‘vote’. The most occurring ‘vote’ is chosen as the representative for the combined model.\n",
    "\n",
    "### Averaging (Regression)\n",
    "It is generally used for regression problems. It involves taking the average of the predictions. The resulting average is used as the overall prediction for the combined model.\n",
    "\n",
    "Using random subsets of data, the risk of overfitting is reduced and flattened by averaging the results of the sub-models. All models are calculated in parallel and then aggregated together afterward.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTtwV5r6Qg9bjepZtHSuGQ.png\" width=600>\n",
    "\n",
    "Now let's have a look on sklearn implementation of **Bagging**\n",
    "\n",
    "We'll use a very common dataset known as Moon Dataset : This is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles.   \n",
    "  \n",
    "To know more about Moon Dataset, visit: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c98fcd",
   "metadata": {},
   "source": [
    "#### Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845abe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X_bagging, y_bagging = make_moons(n_samples=500, noise=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb86cf1",
   "metadata": {},
   "source": [
    "#### Visualize the \"Moons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Plotting the Data\n",
    "plt.scatter(X_bagging[:,0], X_bagging[:,1], c = y_bagging)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b40ca4",
   "metadata": {},
   "source": [
    "#### Train & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac0f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Training & Validation Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_bagging, X_test_bagging, y_train_bagging, y_test_bagging = train_test_split(X_bagging, y_bagging, test_size=0.2, stratify=y_bagging, random_state=1)\n",
    "\n",
    "#Shape of data\n",
    "print(f\"Training => X:{X_train_bagging.shape}, y:{y_train_bagging.shape}\")\n",
    "print(f\"Testing => X:{X_test_bagging.shape}, y:{y_test_bagging.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94623e3",
   "metadata": {},
   "source": [
    "#### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a14ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Individual SVM Classifier\n",
    "svm_bagging = SVC(gamma='auto', kernel=\"rbf\", C=1)\n",
    "svm_bagging_model = svm_bagging.fit(X_train_bagging,y_train_bagging)\n",
    "y_pred_bagging = svm_bagging_model.predict(X_test_bagging)\n",
    "print(\"Individual SVM Model:\", accuracy_score(y_test_bagging,y_pred_bagging))\n",
    "\n",
    "#Ensemble Classification model with SVM as base estimator\n",
    "bag_clf = BaggingClassifier(estimator=svm_bagging_model, n_estimators=20, max_samples=0.8, bootstrap=True)\n",
    "bag_clf.fit(X_train_bagging, y_train_bagging)\n",
    "y_pred_bagging_2 = bag_clf.predict(X_test_bagging)\n",
    "print(\"Ensemble SVM Model:\",accuracy_score(y_test_bagging, y_pred_bagging_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f60ce",
   "metadata": {},
   "source": [
    "Let's configure our **Bagging** classifier\n",
    "\n",
    "- ***estimator***: You have to provide the underlying algorithm that should be used by the random subsets in the bagging procedure in the first parameter. This could be for example Logistic Regression, Support Vector Classification, Decision trees, or many more.\n",
    "- ***n_estimators***: The number of estimators defines the number of bags you would like to create here and the default value for that is 10.\n",
    "- ***max_samples***: The maximum number of samples defines how many samples should be drawn from X to train each base estimator. The default value here is one point zero which means that the total number of existing entries should be used. You could also say that you want only 80% of the entries by setting it to 0.8.\n",
    "\n",
    "After setting the scenes, this model object works like many other models and can be trained using the ``fit()`` procedure including X and y data from the training set. The corresponding predictions on test data can be done using ``predict()``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341196d2",
   "metadata": {},
   "source": [
    "You can clearly observe the difference in their accuracies using this approach. This proves the effectiveness of Bagging. You can prepare a Pasting Model by setting ``bootstrap`` parameter of ``BaggingClassifier`` to \"False\", which means the replacement of instances is not allowed. \n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, thus increasing ensembling efficiency. Overall, in simpler terms Bagging often results in better models, which exaplains why it is generally preferred over pasting.\n",
    "\n",
    "**Note :** The `BaggingClassifier` class supports sampling the features as well. Sampling is controlled by 2 hyperparameters: `max_features` and `bootstrap_features`. They allow random sampling of features for the model with or without repitition. Thus, each predictor can be trained on a random subset of the input features as well. Sampling feature results in even more predictor diversity, hence giving less correlation and making ensemble more effective."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9efa4a80",
   "metadata": {},
   "source": [
    "## 2.1 Voting\n",
    "\n",
    "<img src=\"https://coding-blocks.github.io/DS-NOTES/_images/ensem1.png\" width=\"400\">\n",
    "\n",
    "**Voting** and **Bagging** enseble methods are similar in that they decide on the final result by combining multiple algorithms, but are different in terms of data sampling method. The diagram below may be able to explain their differences effectively.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*E_O_FSHK6SHL6LTNeYaDXw.png\" width=\"500\">\n",
    "\n",
    "As can be seen, algorithms in voting method are trained with the same dataset, but the algorithms in bagging method are trained with different sampled datasets that have been bootstrapped. In other words, algorithms in bagging method are trained with datasets that have been random sampled from the original dataset with replacement. Furthermore, in bagging method, algorithms are identical while different algorithms are combined in voting method. \n",
    "\n",
    "For voting method, there are two methods of performing voting which are **hard voting** and **soft voting**. Hard voting is equivalent to majority vote, and soft voting is essentially averaging out the output of multiple algorithms. Soft voting is usually chosen as the voting method to go. The diagram below shows the mechanism of soft voting.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*NZeYtOSBCzwigkC_QBYhgw.jpeg\" width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "The above ensemble model is also called **Voting Classifier**, as the task performed is of classification and the prediction is made by majority votes. Surprising enough, this isn't the only case. This Voting Classifier **often** (**NOT ALWAYS**) achieves a higher accuracy than the best classifier in the ensemble.\n",
    "\n",
    "### Example: Image Classification\n",
    "\n",
    "The essential concept is encapsulated by means of a didactic illustration involving image classification. Supposing a collection of images, each accompanied by a categorical label corresponding to the kind of animal, is available for the purpose of training a model. In a traditional modeling approach, we would try several techniques and calculate the accuracy to choose one over the other. Imagine we used logistic regression, decision tree, and support vector machines here that perform differently on the given data set.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*RS7jOPKFRPSvg5yW9daWTw.png\" width=\"600\">\n",
    "\n",
    "In the above example, it was observed that a specific record was predicted as a dog by the logistic regression and decision tree models, while a support vector machine identified it as a cat. As various models have their distinct advantages and disadvantages for particular records, it is the key idea of ensemble learning to combine all three models instead of selecting only one approach that showed the highest accuracy.\n",
    "\n",
    "The procedure is called **aggregation** or **voting** and combines the predictions of all underlying models, to come up with one prediction that is assumed to be more precise than any sub-model that would stay alone.\n",
    "\n",
    "Now let's try out ``VotingClassifier`` through Scikit-Learn library.\n",
    "\n",
    "We'll use a dataset about Maternal Health Risk. This dataset contains data that has been collected from different hospitals, community clinics, maternal health cares from the rural areas of Bangladesh through the IoT based risk monitoring system.\n",
    "  \n",
    "To know more about this dataset, visit: https://archive.ics.uci.edu/dataset/863/maternal+health+risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b79e4",
   "metadata": {},
   "source": [
    "#### Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset = pd.read_csv(\"maternal_risk.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c93b9be",
   "metadata": {},
   "source": [
    "#### Encode target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "dataset.RiskLevel = le.fit_transform(dataset.RiskLevel)\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942a650c",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea465b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_maternal_risk = dataset.drop('RiskLevel', axis=1)\n",
    "y_maternal_risk = dataset.RiskLevel\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_maternal_risk = scaler.fit_transform(X_maternal_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b821fd7",
   "metadata": {},
   "source": [
    "#### Train & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ae753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train / Test split\n",
    "X_train_voting, X_test_voting, y_train_voting, y_test_voting = train_test_split(X_maternal_risk, y_maternal_risk, test_size=0.2, stratify=y_maternal_risk, random_state=1)\n",
    "\n",
    "#Shape of data\n",
    "print(f\"Training => X:{X_train_voting.shape}, y:{y_train_voting.shape}\")\n",
    "print(f\"Testing => X:{X_test_voting.shape}, y:{y_test_voting.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=20) # K-Nearest-Neighbours\n",
    "lr = LogisticRegression(random_state=1) # Logistic Regression\n",
    "svm = SVC(gamma='auto', kernel='rbf', C=1, probability=True) # SVM with probability=True\n",
    "#dt = DecisionTreeClassifier(random_state=1) # Decision Tree\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('KNN', knn), ('LR', lr), ('SVM', svm)], voting='hard')\n",
    "voting_clf.fit(X_train_voting,y_train_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for classifier in (knn, lr, svm, voting_clf):\n",
    "    classifier.fit(X_train_voting,y_train_voting)\n",
    "    y_pred_voting = classifier.predict(X_test_voting)\n",
    "    print(classifier.__class__.__name__, \"=>\", accuracy_score(y_test_voting, y_pred_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab21296",
   "metadata": {},
   "source": [
    "In here, the ``VotingClassifier`` achieves a **slight improve in accuracy score** when compared with the scores of the individual models.\n",
    "\n",
    "This is the benefit of **Ensemble Learning**. However the difference in this accuracy will increase with the increase in *uncorrelation* of the error of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c2b86",
   "metadata": {},
   "source": [
    "# 3. Boosting - Reduce Bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc64af27",
   "metadata": {},
   "source": [
    "Boosting is a little variation of the bagging algorithm and uses sequential processing instead of parallel calculations. While bagging aims to reduce the variance of the model, the boosting method tries aims to reduce the bias to avoid underfitting the data. With that idea in mind, boosting also uses a random subset of the data to create an average-performing model on that.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dcSp9TmyJGHzlUPM45hgAQ.png\" width=600>\n",
    "  \n",
    "For that, it uses the miss-classified entries of the weak model with some other random data to create a new model. Therefore, the different models are not randomly chosen but are mainly influenced by wrong classified entries of the previous model. The steps for this technique are the following:\n",
    "\n",
    "1. **Train initial (weak) model**: You create a subset of the data and train a weak learning model which is assumed to be the final ensemble model at this stage. You then analyze the results on the given training data set and can identify those entries that were misclassified.\n",
    "2. **Update weights and train a new model**: You create a new random subset of the original training data but weight those misclassified entries higher. This dataset is then used to train a new model.\n",
    "3. **Aggregate the new model with the ensemble model**: The next model should perform better on the more difficult entries and will be combined (aggregated) with the previous one into the new final ensemble model.\n",
    "\n",
    "There are many boosting methods available, but by far the most popular are **AdaBoost** (short for Adaptive Boosting) and **Gradient Boosting**. Let's start with AdaBooost first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19554c5f",
   "metadata": {},
   "source": [
    "## 3.1 AdaBoost\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is technique used by AdaBoost.  \n",
    "\n",
    "For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such as Decision Tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instances weights, and so on.\n",
    "\n",
    "Consider the error equation :  \n",
    "\n",
    "$\\large{e = \\sum_{i = 1}^n y_i - \\hat{y}_i}$  \n",
    "\n",
    "  \n",
    "> For n = 100 :-  \n",
    "> \n",
    ">> $e = (y_1 - \\hat{y}_1) + (y_2 - \\hat{y}_2) + \\hspace{1mm}...\\hspace{1mm} + \\mathbf{6}(y_{26} - \\hat{y}_{26}) + \\hspace{1mm}...\\hspace{1mm} + \\mathbf{10} (y_{56} - \\hat{y}_{56}) + \\hspace{1mm}...\\hspace{1mm}+(y_{100} - \\hat{y}_{100})$  \n",
    "\n",
    "Suppose my model was misclassifying instance no. **26** & **56**. So I can multiply these instances with some constants, so that my equation for the next model have some extra weight for these points, which means it will contain some bias for these instances, and try to focus on classifying them correctly. This is the base for Adaptive Boosting.  \n",
    "\n",
    "Consider a Classification Dataset:\n",
    "\n",
    "<img src=\"https://coding-blocks.github.io/DS-NOTES/_images/ensem3.png\" width=400>\n",
    "\n",
    "Now if we train an individual Decision Tree model, it’s decision boundary will look something like this, and as we can see it misclassifies a lot of points.\n",
    "\n",
    "<img src=\"https://coding-blocks.github.io/DS-NOTES/_images/ensem4.png\" width=400>\n",
    "\n",
    "So, what we do in Adaptive Boosting is introduce some bias for the misclassified points, so our next model emphasize on them more which will somewhat enhance the decision boundary for our dataset.\n",
    "\n",
    "So, we multiply the terms of these points with some constant that can differ for each point as well, as we use more and more models. Finally, the algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found. To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights. The predicted class is the one that receives the majority of weighted votes.\n",
    "\n",
    "<img src=\"https://coding-blocks.github.io/DS-NOTES/_images/ensem5.png\" width=400>\n",
    "\n",
    "Now, let's have a look on Scikit Learn Implementation of AdaBoost. Scikit-Learn uses a multiclass version of AdaBoost called ***SAMME*** [**S**tagewise **A**dditive **M**odeling using a **M**ulticlass **E**xponential loss function]. When there are just two classes (Binary Classification), SAMME is equivalent to AdaBoost. So let's compare the accuracies of individual decision tree and AdaBoost ensemble of Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fabf38",
   "metadata": {},
   "source": [
    "The code below trains an AdaBoost Classifier based on **200 Decision Trees** using Scikit-Learn's `AdaBoostClassifier` class.\n",
    "\n",
    "In order to check the performance of the AdaBoost, we'll use the dataset Heart Failure Predicition (https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e8f6b",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75eb3ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_heart_failure = pd.read_csv('heart_failure.csv')\n",
    "df_heart_failure.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0c582",
   "metadata": {},
   "source": [
    "#### Enconde all categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_heart_failure['Sex']=le.fit_transform(df_heart_failure['Sex'])\n",
    "df_heart_failure['RestingECG']=le.fit_transform(df_heart_failure['RestingECG'])\n",
    "df_heart_failure['ChestPainType']=le.fit_transform(df_heart_failure['ChestPainType'])\n",
    "df_heart_failure['ExerciseAngina']=le.fit_transform(df_heart_failure['ExerciseAngina'])\n",
    "df_heart_failure['ST_Slope']=le.fit_transform(df_heart_failure['ST_Slope'])\n",
    "\n",
    "df_heart_failure.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287f1f0",
   "metadata": {},
   "source": [
    "#### Create and run the ``AdaBoostClassifier``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_ada = df_heart_failure.drop('HeartDisease', axis=1)\n",
    "y_ada = df_heart_failure.HeartDisease\n",
    "\n",
    "#Train / Test split\n",
    "X_train_ada, X_test_ada, y_train_ada, y_test_ada = train_test_split(X_ada, y_ada, test_size=0.2, stratify=y_ada, random_state=1)\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10, random_state=1), n_estimators=200, random_state=1)\n",
    "ada_clf.fit(X_train_ada, y_train_ada)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train_ada, y_train_ada)\n",
    "\n",
    "y_pred_ensemble = ada_clf.predict(X_test_ada)\n",
    "y_pred_dt = dt.predict(X_test_ada)\n",
    "\n",
    "print(\"Individual Decision Tree Accuracy\", accuracy_score(y_pred_dt, y_test_ada))\n",
    "print(\"AdaBoost Ensemble Accuracy\", accuracy_score(y_pred_ensemble, y_test_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50232064",
   "metadata": {},
   "source": [
    "As you can see the accuracy increased from **~76,1%** to **~83.7%**. As we performed classification task using `AdaBoostClassifier`. Regression tasks can also be performed using `AdaBoostRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47312a",
   "metadata": {},
   "source": [
    "## 3.2 Gradient Boosting\n",
    "\n",
    "Another very popular boosting algorithm is **Gradient Boosting**. Just like AdaBoost, Gradient Boosting works sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the *residual errors* made by the previous predictor. The concept of Gradient Boosting can be understood easily by taking an example.  \n",
    "  \n",
    "Consider the case of Linear Regression.  \n",
    "\n",
    "> $e = y - \\hat{y}$  \n",
    "  \n",
    "Let's take our I$^{st}$ model and put this equation.  \n",
    "\n",
    "> $e_1 = y - \\hat{y}_1$  \n",
    "  \n",
    "Suppose it returned some error $e_1$, so now we'll train our next model on this error like:  \n",
    "  \n",
    "> $e_2 = e_1 - \\hat{e}_1\\hspace{1.5cm}$ or we can say  \n",
    ">   \n",
    "> $e_2 = y - \\hat{y}_1 - \\hat{e}_1$  \n",
    "  \n",
    "Compare above equation with $e = y - \\hat{y}$ , we can say  \n",
    "  \n",
    "> $\\hat{y} = \\hat{y}_1 + \\hat{e}_1$  \n",
    "  \n",
    "For Mutiple Models:  \n",
    "\n",
    "> $\\hat{y} = \\hat{y}_1 + \\hat{e}_1 + \\hat{e}_2 + \\hat{e}_3 + \\hat{e}_4 + ...$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a3423",
   "metadata": {},
   "source": [
    "**Writing this programatically:**\n",
    "\n",
    "We'll use the BMW Price prediciton dataset - https://www.kaggle.com/datasets/danielkyrka/bmw-pricing-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bdd849",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmw = pd.read_csv('bmw_pricing_challenge.csv')\n",
    "df_bmw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65eebe5",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bmw = df_bmw.drop(['maker_key', 'sold_at'], axis=1, errors='ignore')\n",
    "df_bmw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d24d6d",
   "metadata": {},
   "source": [
    "#### Encode all categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_bmw['model_key']=le.fit_transform(df_bmw['model_key'])\n",
    "df_bmw['fuel']=le.fit_transform(df_bmw['fuel'])\n",
    "df_bmw['paint_color']=le.fit_transform(df_bmw['paint_color'])\n",
    "df_bmw['car_type']=le.fit_transform(df_bmw['car_type'])\n",
    "\n",
    "# Convert registration_date to integer\n",
    "df_bmw['registration_date'] = df_bmw['registration_date'].str.replace(\"-\",\"\").astype(int)\n",
    "\n",
    "df_bmw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031c996",
   "metadata": {},
   "source": [
    "#### Manual Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "X_gb = df_bmw.drop('price', axis=1, errors='ignore')\n",
    "y_gb = df_bmw.price\n",
    "\n",
    "#Train / Test split\n",
    "X_train_gb, X_test_gb, y_train_gb, y_test_gb = train_test_split(X_gb, y_gb, test_size=0.2, random_state=1)\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=20, random_state=1)\n",
    "tree_reg1.fit(X_train_gb, y_train_gb)\n",
    "\n",
    "e1 = y_train_gb - tree_reg1.predict(X_train_gb)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=20, random_state=1)\n",
    "tree_reg2.fit(X_train_gb, e1)\n",
    "\n",
    "e2 = e1 - tree_reg2.predict(X_train_gb)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=20, random_state=1)\n",
    "tree_reg3.fit(X_train_gb, e2)\n",
    "\n",
    "y_pred_gb_manual = sum(tree.predict(X_test_gb) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test_gb, y_pred_gb_manual))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test_gb, y_pred_gb_manual))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test_gb, y_pred_gb_manual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605aa3e",
   "metadata": {},
   "source": [
    "The above code can be fine-tuned by using the Sklearn's ``GradientBoostingRegressor`` implementation.\n",
    "\n",
    "#### Sklearn ``GradientBoostingRegressor`` implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aed926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "gbr = GradientBoostingRegressor(\n",
    "    max_depth=20, \n",
    "    n_estimators=3, \n",
    "    loss='absolute_error', \n",
    "    learning_rate=0.6,\n",
    "    random_state=1\n",
    ")\n",
    "gbr.fit(X_train_gb, y_train_gb)\n",
    "\n",
    "y_pred_gb_sklearn = gbr.predict(X_test_gb)\n",
    "\n",
    "print('Mean Absolute Error:', mean_absolute_error(y_test_gb, y_pred_gb_sklearn))\n",
    "print('Mean Squared Error:', mean_squared_error(y_test_gb, y_pred_gb_sklearn))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test_gb, y_pred_gb_sklearn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301567f",
   "metadata": {},
   "source": [
    "Now let’s try to understand the graphical representation of the error also (Refer to the figure below):\n",
    "\n",
    "<img src=\"https://coding-blocks.github.io/DS-NOTES/_images/ensem6.png\" width=600>\n",
    "\n",
    "* The figure represents the predictions of these three trees in the left column, and the ensemble’s prediction in the right column. \n",
    "* In the first row, the ensemble has just one tree, so its predictions are exactly the same as the first tree’s predictions. \n",
    "* In the second row, a new tree is trained on the residual errors of the first tree. \n",
    "* On the right you can see that the ensemble’s predictions are equal to the sum of the predictions of the first two trees. * Similarly, in the third row another tree is trained on the residual errors of the second tree. You can see that the ensemble’s predictions gradually get better as trees are added to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb88b8",
   "metadata": {},
   "source": [
    "The same way Gradient Boosting can also be used for classification tasks. You can import it by ``GradientBoostingClassifier`` under the ensemble module of scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc056b6",
   "metadata": {},
   "source": [
    "# 4. Conclusion: Bagging vs. Boosting \n",
    "\n",
    "Bagging and boosting are important for ensuring the accuracy of models. They can help prevent undesirable consequences caused by inaccurate models. Below are some of the key takeaways from the article:\n",
    "\n",
    "Ensemble learning combines multiple machine learning models into a single model. The aim is to increase the performance of the model.\n",
    "\n",
    "* **Bagging** aims to **decrease variance**, **boosting** aims to **decrease bias**.\n",
    "* **Bagging** and **boosting** combine **homogenous weak learners**.\n",
    "* **Bagging** trains models in **parallel** and **boosting** trains the models **sequentially**.\n",
    "\n",
    "The table below shows the similarities and the differences between the ensemble methods.\n",
    "\n",
    "<img src=\"https://pluralsight2.imgix.net/guides/56a97436-3f9d-47b0-9a09-2dfadef5253d_8.jpg\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
