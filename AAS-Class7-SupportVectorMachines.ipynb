{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machines (SVM) is a binary linear classification whose decision boundary is explicitly constructed to minimize generalization error. It is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression and even outlier detection. \n",
    "\n",
    "SVM is well suited for classification of complex but small or medium sized datasets.\n",
    "\n",
    "**In other words...**\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Support Vectors?\n",
    "\n",
    "![alt text](https://www.dtreg.com/uploaded/pageimg/SvmMargin2.jpg \"Logo Title Text 1\")\n",
    " \n",
    "Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set, they are what help us build our SVM. \n",
    "\n",
    "## Whats a hyperplane?\n",
    "\n",
    "![alt text](http://slideplayer.com/slide/1579281/5/images/32/Hyperplanes+as+decision+surfaces.jpg \"Logo Title Text 1\")\n",
    "\n",
    "Geometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with dimension n − 1. By its nature, it separates the space into two half spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate two classes\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/540/0*9jEWNXTAao7phK-5.png) ![alt text](https://cdn-images-1.medium.com/max/540/0*0o8xIA4k3gXUDCFU.png)\n",
    "\n",
    "![alt text](https://miro.medium.com/v2/resize:fit:1100/1*XE9jt0r1yAW8LnliQ3mllQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does SVM classify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to start with the intuition for SVM with the **special linearly separable** classification case.\n",
    "\n",
    "If classification of observations is **\"linearly separable\"**, SVM fits the **\"decision boundary\"** that is defined by the largest margin between the closest points for each class. This is commonly called the **\"maximum margin hyperplane (MMH)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linearly separable SVM](https://raw.githubusercontent.com/nalamidi/Breast-Cancer-Classification-with-Support-Vector-Machine/master/linear_separability_vs_not.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Classify Tumors\n",
    "\n",
    "   Classify tumors into malignant (cancer) or benign using features obtained from several cell images.\n",
    "   \n",
    "   Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "   \n",
    "   \n",
    "**Attribute Information:**\n",
    "   \n",
    "1.  ID number \n",
    "2.  Diagnosis (M = malign, B = benign) \n",
    "\n",
    "**Ten real-valued features are computed for each cell nucleus:**\n",
    "\n",
    "1. Radius (mean of distances from center to points on the perimeter) \n",
    "2. Texture (standard deviation of gray-scale values) \n",
    "3. Perimeter \n",
    "4. Area \n",
    "5. Smoothness (local variation in radius lengths) \n",
    "6. Compactness (perimeter^2 / area - 1.0) \n",
    "7. Concavity (severity of concave portions of the contour) \n",
    "8. Concave points (number of concave portions of the contour) \n",
    "9. Symmetry \n",
    "10. Fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import needed Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Import Cancer data from the Sklearn library\n",
    "# Dataset can also be found here (http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29)\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "cancer = load_breast_cancer()\n",
    "cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's view the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df_cancer['target'] = pd.Series(cancer.target)\n",
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Explore Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cancer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see,we have 596 rows (Instances) and 31 columns(Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the name of each columns in our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next step is to Visualize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot out just the first 5 variables (features)\n",
    "sns.pairplot(df_cancer, vars = ['mean radius', 'mean texture', 'mean perimeter', 'mean area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots shows the relationship between our features. But the only problem with them is that they do not show us which of the \"dots\" is Malignant and which is Benign. \n",
    "\n",
    "This issue will be addressed below by using \"target\" variable as the \"hue\" for the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot out just the first 5 variables (features)\n",
    "sns.pairplot(df_cancer, hue = 'target', vars = ['mean radius', 'mean texture', 'mean perimeter','mean area'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "    \n",
    "  1.0 (Orange) = Benign (No Cancer)\n",
    "  \n",
    "  0.0 (Blue) = Malignant (Cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many Benign and Malignant do we have in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancer['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have 212 - Malignant, and 357 - Benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's visulaize our counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df_cancer['target'], label = \"Count\") # TO FIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check the correlation between our features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12)) \n",
    "sns.heatmap(df_cancer.corr(), annot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong correlation between the mean radius and mean perimeter, mean area and mean primeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From our dataset, let's create the target and predictor matrix**\n",
    "\n",
    "- \"y\" = Is the feature we are trying to predict (Output). In this case we are trying to predict wheither our \"target\" is Cancer (Malignant) or not (Benign). I.e. we are going to use the \"target\" feature here.\n",
    "- \"X\" = The predictors which are the remaining columns (mean radius, mean texture, mean perimeter, mean area, mean smoothness, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_cancer.drop(['target'], axis = 1) # We drop our \"target\" feature and use all the remaining features in our dataframe to train the model.\n",
    "#X.head()\n",
    "#X = df_cancer.drop(['target'], axis = 'columns')\n",
    "#X.head()\n",
    "\n",
    "X = df_cancer.iloc[:, 0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cancer['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've assigned values to our \"X\" and \"y\", the next step is to import the python library that will help us to split our dataset into training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training data = Is the subset of our data used to train our model.\n",
    "- Testing data =  Is the subset of our data that the model hasn't seen before. This is used to test the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our data using 80% for training and the remaining 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final consideration is for **classification problems only**.\n",
    "\n",
    "Some classification problems do not have a balanced number of examples for each class label. As such, it is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset.\n",
    "\n",
    "This is called a stratified **train-test split**.\n",
    "\n",
    "We can achieve this by setting the “**stratify**” argument to the \"y\" component (**target variable**) of the original dataset. This will be used by the train_test_split() function to ensure that both the train and test sets have the proportion of examples in each class that is present in the provided “y” array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10, stratify=y)\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let now check the size our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The size of our training \"X\" (input features - X_train) is', X_train.shape)\n",
    "print ('The size of our testing \"X\" (input features - X_test) is', X_test.shape)\n",
    "print ('The size of our training \"y\" (output feature - Y_train) is', y_train.shape)\n",
    "print ('The size of our testing \"y\" (output features - Y_test) is', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Support Vector Machine (SVM) Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tambem sera possivel fazer regressões com SVM - Support Vector Regressor\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    " # Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Try other kernels (\"rbf\" - Radial Based Function, \"linear\" - Linear and \"poly\" - Polynomial)\n",
    "# C = Regularization\n",
    "svc_algo = SVC(gamma=\"auto\", kernel=\"rbf\", C=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps you to build a more accurate classifier.\n",
    "\n",
    "The most used Kernels are:\n",
    "\n",
    "- **Linear Kernel** A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "\n",
    "\n",
    "- **Polynomial Kernel** A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.\n",
    "\n",
    "\n",
    "- **Radial Basis Function Kernel** The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space.\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-866b6450fc8c20dbf5dbd6a404cfe58a\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "\n",
    "- **Kernel:** The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n",
    "\n",
    "\n",
    "- **Regularization:** Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n",
    "\n",
    "\n",
    "- **Gamma**: A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting. In other words, you can say a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma considers all the data points in the calculation of the separation line.\n",
    "\n",
    "<img src=\"https://amueller.github.io/COMS4995-s18/slides/aml-09-021418-support-vector-machines/images/img_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's train our SVM model with our \"training\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_model.fit vai treinar o nosso modelo com o subset treino (X_train, y_train)\n",
    "model = svc_algo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use our trained model to make a prediction using our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previsoes efetuadas pelo nosso modelo (utilizando x_test e guardando as previsoes numa varialvel - y_predict)\n",
    "y_predict = model.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next step is to check the accuracy of our prediction by comparing it to the output we already have (y_test). We are going to use confusion matrix for this comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metric libraries\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predict, labels=[0,1])\n",
    "confusion = pd.DataFrame(cm, index=['is_cancer', 'is_not_cancer'],\n",
    "                         columns=['predicted_cancer','predicted_not_cancer'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, our model did not do a very good job in its predictions.**\n",
    "\n",
    "**One way of improving the model is by changing kernels and its hyperparameters.** \n",
    "\n",
    "But, let's explore another ways to improve the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first process we will try is by Normalizing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Normalization is a feature scaling process that brings all values into range [0,1], or between [-1,1] or between any interval, which depends on the used scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the scaling between [0,1], it applies the following formula**:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/682/0*8btMQlMD6O50pUDP\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we do and don't normalize data:\n",
    "\n",
    "<img src=\"https://scikit-learn.org/0.18/_images/sphx_glr_plot_robust_scaling_001.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to normalize, we used a method from Sciki-learn called **MinMaxScaler**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crio o scaler (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Fit and Transform (fit vai decidir os valores, e o transform vai retornar esses valores)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# criamos um novo dataset de treino, mas desta vez escalado\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_2 = MinMaxScaler()\n",
    "#scaler_2 = StandardScaler()\n",
    "X_test_scaled = scaler_2.fit_transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "X_test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_algo_2 = SVC(gamma=\"auto\", kernel=\"sigmoid\", C=9)\n",
    "model = svc_algo_2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_predict_2 = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_predict_2, labels=[0,1])\n",
    "confusion = pd.DataFrame(cm, index=['is_cancer', 'is_not_cancer'],\n",
    "                         columns=['predicted_cancer','predicted_is_not_cancer'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_predict_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Awesome performance! We only have 1 false prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize using 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: http://bit.ly/2iv7FFL\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def plot_model(model, x1, x2, title):\n",
    "\n",
    "    X0, X1 = X.iloc[:, 0], X.iloc[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    \n",
    "    plt.figure()    \n",
    "    plot_contours(plt, model, xx, yy, alpha=0.75)\n",
    "    plt.scatter(X0, X1, c=y, s=15, alpha=0.95, edgecolors='#333333', linewidths=0.3) \n",
    "    plt.xlabel(x1)\n",
    "    plt.ylabel(x2)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[:,:2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "svclf = SVC(C=9, kernel=\"rbf\", gamma='auto')\n",
    "svclf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = svclf.predict(X_test)\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 'Mean Radius'\n",
    "x2 = 'Mean Texture'\n",
    "title = 'SVM with RBF Kernel'\n",
    "\n",
    "plot_model(svclf, x1, x2, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prever para valores diferentes de mean_radius e mean_texture\n",
    "X_test_2 = [\n",
    "    [50.10, 10.52], \n",
    "    [50.50, 10.1]\n",
    "]\n",
    "svclf.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do better by normalizing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the new training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_3 = MinMaxScaler()\n",
    "X_train_scaled_2 = scaler_3.fit_transform(X_train)\n",
    "X_train_scaled_2 = pd.DataFrame(X_train_scaled_2, columns=X_train.columns)\n",
    "X_train_scaled_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the new test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_4 = MinMaxScaler()\n",
    "X_test_scaled_2 = scaler_4.fit_transform(X_test)\n",
    "X_test_scaled_2 = pd.DataFrame(X_test_scaled_2, columns=X_test.columns)\n",
    "X_test_scaled_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclf2 = SVC(kernel=\"rbf\", gamma='auto')\n",
    "svclf2.fit(X_train_scaled_2, y_train)\n",
    "\n",
    "y_predict_2 = svclf2.predict(X_test_scaled_2)\n",
    "print(classification_report(y_test,y_predict_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yes! We did a little bit better. Normalizing data works!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons associated with SVM\n",
    "\n",
    "### Pros:\n",
    "- It works really well with clear margin of separation\n",
    "- It is effective in high dimensional spaces.\n",
    "- It is effective in cases where number of dimensions is greater than the number of samples.\n",
    "- It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "### Cons:\n",
    "- It doesn’t perform well, when we have large data set because the required training time is higher\n",
    "- It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping\n",
    "- SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. - - It is related SVC method of Python scikit-learn library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
