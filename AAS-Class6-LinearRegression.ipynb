{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Linear Regression</u>\n",
    "\n",
    "# 1. Correlation vs Linear Regression\n",
    "\n",
    "Correlation analysis concerns the degree of a relationship between two variables. The result is a value between -1.0 and 1.0, where 1.0 denotes the highest positive correlation, -1.0 stands for the highest negative correlation, and 0 means no correlation.\n",
    "\n",
    "Example: Is there any relationship between purchasing an outdoor concert ticket for the next week-end and weather predictions?\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2c4dd36d-ef1e-45af-8499-b94875043d75_1600x431.png\" width=\"900\" height=\"900\"> [Berezovsky, 2023]\n",
    "\n",
    "Correlation analysis is related to the relationship and connection between two variables.\n",
    "\n",
    "Linear regression is a function. It's analysis shows how much one variable affects another and how to predict, estimate, or explain its behavior.\n",
    "\n",
    "If your analysis aims to answer if there is a relationship between X and Y, use correlation. If you aim to answer how X affects Y or have X predict Y, use regression.\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62413fa0-3d80-411c-af93-ebd0f096a26a_1042x644.png\" width=\"500\" height=\"500\"> [Berezovsky, 2023]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression\n",
    "\n",
    "Linear regression analysis is used to predict the value of a variable based on the value of another variable (simple linear regression) or variables (multiple linear regression). \n",
    "\n",
    "The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*N1-K-A43_98pYZ27fnupDA.jpeg\" width=\"700\" height=\"700\"> [Wong, 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Linear Regression in Machine Learning\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Baptiste-Caramiaux/publication/270818818/figure/fig1/AS:392191798202396@1470517219585/Regression-example-of-a-simple-linear-regression-Input-Output-relationship-is-modeled.png\" width=\"500\" height=\"500\"> [Caramiaux & Tanaka, 2013]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example: Advertising Data\n",
    "\n",
    "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!\n",
    "\n",
    "[James et al., 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Scikit-learn: a software machine learning library used for Linear Regression analysis\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data into a DataFrame\n",
    "data = pd.read_csv('Advertising.csv', index_col=0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the <span style=\"color:red\">**features**</span>?\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the <span style=\"color:red\">**response**</span>?\n",
    "- Sales: sales of a single product in a given market (in thousands of widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the DataFrame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 200 **observations**, and thus 200 markets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "fig, axs = plt.subplots(1, 3, sharey=True, figsize=(12, 6))\n",
    "data.plot(kind='scatter', x='TV', y='Sales', ax=axs[0])\n",
    "data.plot(kind='scatter', x='Radio', y='Sales', ax=axs[1])\n",
    "data.plot(kind='scatter', x='Newspaper', y='Sales', ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions About the Advertising Data\n",
    "\n",
    "Let's pretend you work for the company that manufactures and markets this widget. The company might ask you the following: On the basis of this data, <u>how should we spend our advertising money in the future</u>?\n",
    "\n",
    "This general question might lead you to more specific questions:\n",
    "1. Is there a <u>relationship</u> between ads and sales?\n",
    "2. How strong is that relationship?\n",
    "3. Which ad types contribute to sales?\n",
    "4. What is the effect of each ad type of sales?\n",
    "5. Given ad spending in a particular market, can sales be <u>predicted</u>?\n",
    "\n",
    "We will explore these questions below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple Linear Regression\n",
    "\n",
    "<b>Simple linear regression</b> is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following formula:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x$    \n",
    "\n",
    "What does each term represent?\n",
    "- $y$ is the response\n",
    "- $x$ is the feature\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. To create your model, you must \"<b>learn</b>\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict Sales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we find the line which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ab2757/Financial_Analytics/master/LinearRegression/Images/08_estimating_coefficients.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction of those calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ab2757/Financial_Analytics/master/LinearRegression/Images/08_slope_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use **Scikit-learn** to estimate the model coefficients for the advertising data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "#feature_cols = ['TV']\n",
    "X = data[['TV']].values\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "model = linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "# Beta0\n",
    "print(linreg.intercept_)\n",
    "\n",
    "# Beta1\n",
    "print(linreg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Model Coefficients\n",
    "\n",
    "How do we interpret the TV coefficient ($\\beta_1$)?\n",
    "- <span style=\"color:red\">A \"unit\" increase in TV ad spending is **associated with** a 0.047537 \"unit\" increase in Sales</span>.\n",
    "- Or more clearly: <span style=\"color:red\">An additional $1,000 spent on TV ads is **associated with** an increase in sales of 47.537 widgets</span>.\n",
    "\n",
    "Note that if an increase in TV ad spending was associated with a **decrease** in sales, $\\beta_1$ would be **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model for Prediction\n",
    "\n",
    "Let's say that there was a new market where the TV advertising spend was **$50,000**. What would we predict for the Sales in that market? Let's apply the simple linear regression formula.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x$$\n",
    "$$y = 7.032594 + 0.047537 \\times 50$$\n",
    "$$y=7.032593549127693 + 0.047536640433019764*50$$\n",
    "$$y=9,409425571$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Scikit-learn to make the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[50], [200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help interpreting the result, check the Scikit-learn reference for the LinearRegression predict function:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict\n",
    "\n",
    "If there was a new market where the TV advertising spend was **$50,000**, we predict Sales of **9,409 widgets** in that market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Least Squares Line\n",
    "\n",
    "Let's make predictions for the **smallest and largest observed values of x**, and then use the predicted values to plot the least squares line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = model.predict(X)\n",
    "\n",
    "# first, plot the observed data\n",
    "#data.plot(kind='scatter', x='TV', y='Sales')\n",
    "plt.scatter(data['TV'], data['Sales'])\n",
    "\n",
    "# then, plot the least squares line\n",
    "plt.plot(data['TV'], preds, c='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Well Does the Model Fit the data?\n",
    "\n",
    "The most common way to <u>evaluate the overall fit</u> of a linear model is by the **R-squared** value. <span style=\"color:red\">R-squared is the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model</span>, or the reduction in error over the **null model**. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)\n",
    "\n",
    "R-squared is between 0 and 1, and higher is better because it means that more variance is explained by the model. Here's an example of what R-squared \"looks like\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ab2757/Financial_Analytics/master/LinearRegression/Images/08_r_squared.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the **blue line** explains some of the variance in the data (R-squared=0.54), the **green line** explains more of the variance (R-squared=0.64), and the **red line** fits the training data even further (R-squared=0.66). \n",
    "\n",
    "Let's calculate the R-squared value for our simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the R-squared value for the model (by default it uses the R-squared score)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is that a \"good\" R-squared value?**\n",
    "\n",
    "- It's hard to say\n",
    "- The threshold for a good R-squared value depends widely on the domain\n",
    "- Therefore, it's most useful as a tool for **comparing different models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multiple Linear Regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features, which is called <span style=\"color:red\">**multiple linear regression**</span>:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n",
    "\n",
    "Let's estimate these coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols].values\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate and fit\n",
    "linreg2 = LinearRegression()\n",
    "model = linreg2.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the feature names with the coefficients\n",
    "list(zip(feature_cols, linreg2.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For a given amount of Radio and Newspaper spending, an increase of \\$1000 in **TV** spending is associated with an **increase in Sales of 45.8 widgets**\n",
    "- For a given amount of TV and Newspaper spending, an increase of \\$1000 in **Radio** spending is associated with an **increase in Sales of 188.5 widgets**.\n",
    "- For a given amount of TV and Radio spending, an increase of \\$1000 in **Newspaper** spending is associated with an **decrease in Sales of 1.0 widgets**. How could that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for a new observation\n",
    "linreg2.predict([[100, 25, 25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the R-squared\n",
    "linreg2.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issure with R-squared**\n",
    "\n",
    "- **R-squared will always increase as you add more features to the model**, even if they are unrelated to the response\n",
    "    - Selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "- **Adjusted R-squared**\n",
    "    - Penalizes model complexity (to control for overfitting), but it generally under-penalizes complexity.\n",
    "\n",
    "**Better Solution**\n",
    "\n",
    "- **Train/test split or cross-validation**\n",
    "    - More reliable estimate of out-of-sample error\n",
    "    - Better for choosing which of your models will best generalize to out-of-sample data\n",
    "    - There is extensive functionality for cross-validation in scikit-learn, including automated methods for searching different sets of parameters and different models\n",
    "    - Importantly, cross-validation can be applied to any model, whereas the methods described above only apply to linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation metrics for regression problems\n",
    "\n",
    "Evaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. We need evaluation metrics designed for comparing **continuous values**.\n",
    "\n",
    "Let's create some example numeric predictions, and calculate <u>three common evaluation metrics for regression problems</u>:\n",
    "#### Define true and predicted response values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y\n",
    "y_pred = linreg2.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    " **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAE, MSE, RMSE\n",
    "print('Mean Absolute Error: ', metrics.mean_absolute_error(y_true, y_pred))\n",
    "print('Mean Squared Error: ', metrics.mean_squared_error(y_true, y_pred))\n",
    "print('Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing these metrics:\n",
    "- **MAE** is the easiest to understand, because it's the average error.\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "#### All of these are **loss functions**, because we want to minimize them.\n",
    "\n",
    "Here's an additional example, to demonstrate how MSE/RMSE punish larger errors:\n",
    "\n",
    "#### Same true values as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [100, 50, 30, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New set of predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [10, 10, 10, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE is larger than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using train/test split (or cross-validation)\n",
    "\n",
    "### A better approach to feature selection!\n",
    "- They attempt to directly estimate how well your model will **generalize** to out-of-sample data.\n",
    "- They rely on **fewer assumptions** that linear regression.\n",
    "- They can easily be applied to **any model**, not just linear models.\n",
    "\n",
    "### define a function that accepts X and y and computes testing RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_rmse(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    linreg = LinearRegression()\n",
    "    model = linreg.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "def train_test_mae(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    linreg = LinearRegression()\n",
    "    model = linreg.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return metrics.mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "train_test_rmse(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, is there any chance that we can lower the RMSE value? We can try exclude the least correlated features. Let's try to see which ones are correlated the least:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Seaborn visualization library\n",
    "import seaborn as sns\n",
    "\n",
    "# plot the heatmap\n",
    "data_correlations = data.corr()\n",
    "sns.heatmap(data_correlations, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Exclude Newspaper !</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['TV', 'Radio']\n",
    "X = data[feature_cols]\n",
    "train_test_rmse(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Voil√°! A lower RMSE as expected!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Comparing linear regression with other models\n",
    "\n",
    "## Advantages of linear regression:\n",
    "- Simple to explain\n",
    "- Highly interpretable\n",
    "- Model training and prediction are fast\n",
    "- No tuning is required (excluding regularization)\n",
    "- Features don't need scaling\n",
    "- Can perform well with a small number of observations\n",
    "\n",
    "## Disadvantages of linear regression:\n",
    "- Presumes a linear relationship between the features and the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "- Sensitive to irrelevant features\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Bibliography\n",
    "\n",
    "\n",
    "[Berezovsky, 2023] Olga Berezovsky, \"How to do linear regression and correlation analysis\", Lenny's Newsletter, available at https://www.lennysnewsletter.com/p/linear-regression-and-correlation-analysis\n",
    "\n",
    "[Caramiaux & Tanaka, 2013] Baptiste Caramiaux, Atau Tanaka, \"Machine Learning of Musical Gestures\", Proceedings of the International Conference on New Interfaces For Musical Expression (NIME), available at https://www.researchgate.net/publication/270818818_Machine_Learning_of_Musical_Gestures\n",
    "\n",
    "[James et al., 2021], Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, \"An Introduction to Statistical Learning: with Applications in R, 2021 Edition\", Ed.Springer, available at https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6009dd9fa7bc363aa822d2c7/1611259312432/ISLR+Seventh+Printing.pdf\n",
    "\n",
    "[Wong, 2020] Jason Wong, \"Linear Regression Explained\", Medium, available at https://towardsdatascience.com/linear-regression-explained-1b36f97b7572 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
